# Aithena Services

<p align="center">
  <img src="https://img.shields.io/badge/version-1.0.0--dev0-blue" alt="Version 1.0.0-dev0">
  <img src="https://img.shields.io/badge/license-MIT-green" alt="License: MIT">
</p>

> **Complete AI Stack with Powerful Memory Capabilities**

## üöÄ Building the Complete AI Development Environment

Aithena Services is part of a powerful, integrated AI development stack that brings together local and cloud LLMs, vector memory, and database functionality through a unified API. This complete solution enables you to build sophisticated AI applications with minimal setup and maximum flexibility.

While Aithena Services specializes in robust vector memory and database functionality, it's designed to work as part of a complete ecosystem that includes LiteLLM for model access, Ollama for local LLMs, and vector database capabilities ‚Äî all through a consistent, OpenAI-compatible API.

### üß† The Memory Layer for Your AI Applications

Aithena Services handles the storage and retrieval of information while integrating perfectly with language model providers, letting you build more sophisticated AI solutions with long-term memory capabilities.

**‚ö° Looking for quick deployment?** Check out our [Docker Compose Setup](docs/docker_compose.md) to get a complete AI stack running in minutes.

## üèóÔ∏è System Architecture

<p align="center">
  <img src="docs/resources/architecture.svg" alt="Aithena Services Architecture" width="800">
</p>

The diagram above shows how Aithena Services fits into a complete AI stack, providing vector memory capabilities that integrate with LiteLLM for a unified solution. üß© Each component plays a vital role in the complete ecosystem.

## ‚ú® Key Features

- **Vector Database**: Built-in PostgreSQL/pgvector integration for efficient vector storage and similarity search
- **Seamless Integration**: Works perfectly with LiteLLM for a complete AI stack
- **Memory API**: Clean, well-documented API for storing and retrieving vector embeddings
- **Docker Ready**: Deploy as a standalone service or as part of a complete stack
- **Optimized Performance**: Efficient cosine similarity search for finding relevant content

## ü§î Why Choose Aithena Services?

- **Focus on Memory**: Specialized in vector storage and retrieval, doing one thing exceptionally well
- **Cloud-Agnostic**: Works with any LLM provider through LiteLLM integration
- **Simple API**: Clean, consistent interface for all memory operations
- **Production Ready**: Designed for reliability and performance in production environments
- **Active Development**: Constantly improving with new features and optimizations

## üê≥ Recommended Deployment

**We strongly recommend using our Docker Compose stack** for the best experience with Aithena Services. This approach gives you a complete, pre-configured AI development environment with just a few commands.

```bash
# Configure and start
cp .env.sample .env
cp config_sample.yaml config.yaml
# Edit .env and config.yaml with your settings
docker compose up -d
```

For detailed setup instructions, see our [Docker Compose guide](docs/docker_compose.md).

### üöÄ Why Docker Compose is Better

Our Docker Compose stack provides:

- **Unified API Gateway**: Connect to multiple LLM providers (OpenAI, Anthropic, Claude, Groq, etc.) through a single API
- **Integrated Memory Layer**: Seamless vector storage for building apps with long-term memory and context
- **Embedding Generation**: Built-in support for creating and storing embeddings from various providers
- **Local Model Support**: Run open-source models locally with Ollama integration
- **Pre-configured Components**: All services are pre-configured to work together perfectly
- **One-command Deployment**: Get your entire AI stack running with a single command

The complete stack includes:

- **Aithena Services**: Memory/vector functionality
- **LiteLLM**: OpenAI-compatible API for model access
- **Ollama**: Local model hosting
- **PGVector**: Vector database
- **LiteLLM UI**: Web dashboard for monitoring and management

While you can run Aithena Services as a standalone memory component, the full stack delivers a significantly more powerful developer experience.

## üîç Quick Usage Examples

### Store Vector Embeddings

```bash
curl -X POST http://localhost:8000/memory/pgvector/insert \
  -H "Content-Type: application/json" \
  -d '{
    "table_name": "my_documents",
    "id": "doc1",
    "vector": [0.1, 0.2, 0.3, ...],
    "metadata": {"title": "Important Document", "content": "This contains key information"}
  }'
```

### Search for Similar Content

```bash
curl -X POST http://localhost:8000/memory/pgvector/search \
  -H "Content-Type: application/json" \
  -d '{
    "table_name": "my_documents",
    "vector": [0.1, 0.2, 0.3, ...],
    "n": 5
  }'
```

## üìö Documentation

Comprehensive documentation is available to help you get started:

- [Quick Start Guide](docs/quickstart.md)
- [Docker Compose Setup](docs/docker_compose.md)
- [API Reference](docs/api.md)
- [Memory Features](docs/memory.md)
- [Environment Variables](docs/env.md)
- [Project Structure](docs/structure.md)

## üîå Integration with LiteLLM

When deployed with Docker Compose, Aithena Services integrates perfectly with LiteLLM through a convenient passthrough, allowing you to:

1. Access vector memory at `http://localhost:4000/memory/...`
2. Use LLM functionality at `http://localhost:4000/chat/completions`

This means your application only needs to talk to a single API endpoint for both memory and LLM functionality.

## üë• Community and Support

- **GitHub Issues**: Report bugs or request features
- **Contributions**: Pull requests are welcome
- **Documentation**: Detailed examples and guides available

## üìÑ License

Aithena Services is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
